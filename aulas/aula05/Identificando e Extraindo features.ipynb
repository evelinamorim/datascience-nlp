{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt, mpld3 \n",
    "# The mpld3 project brings together Matplotlib, the popular Python-based graphing \n",
    "#library, and D3js, the popular JavaScript library for creating interactive data visualizations for the web.\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "import spacy\n",
    "# precisa instalar e baixar antes: \n",
    "# para windows procurar hunspell em: https://sourceforge.net/projects/ezwinports/files/\n",
    "# e depois colocar o executável no PATH do windows\n",
    "# e só depois pip install hunspell\n",
    "import hunspell \n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# Dados relativos ao artigo: https://sites.icmc.usp.br/taspardo/PROPOR2018-MonteiroEtAl.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_arquivos(data_dir):\n",
    "    \n",
    "    lst_files = []\n",
    "    for dirpath, dirnames, filenames in os.walk(data_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith(\".txt\"):\n",
    "                lst_files.append(os.path.join(dirpath, f))\n",
    "                \n",
    "    return lst_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando os arquivos fakes\n",
    "lst_fake = corpus_arquivos(\"Fake.Br Corpus/full_texts/fake/\")\n",
    "\n",
    "# pegando os arquivos verdadeiros\n",
    "lst_verdadeiros = corpus_arquivos(\"Fake.Br Corpus/full_texts/true/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n"
     ]
    }
   ],
   "source": [
    "# leitura dos arquivos \n",
    "# classes 1: fake / 0: true\n",
    "# encoding: utf-8 (universal) e iso8859-1 (português)\n",
    "target = []\n",
    "file_names = []\n",
    "text_news = []\n",
    "for news_file in lst_fake:\n",
    "    file_names.append(news_file)\n",
    "    text_news.append(codecs.open(news_file, \"r\",encoding=\"iso8859-1\").read())\n",
    "    target.append(1)\n",
    "    \n",
    "#text_news = text_news[:100]\n",
    "#target = target[:100]\n",
    "#file_names = file_names[:100]\n",
    "\n",
    "for news_file in lst_verdadeiros:\n",
    "    file_names.append(news_file)\n",
    "    text_news.append(codecs.open(news_file, \"r\", encoding=\"iso8859-1\").read())\n",
    "    target.append(0)\n",
    "    \n",
    "#text_news = text_news[:200]\n",
    "#target = target[:200]\n",
    "#file_names = file_names[:200]\n",
    "\n",
    "print(len(text_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# podemos remover '' e `` tambem, eles apareceram como muito frequentes em \n",
    "# uma primeira execucao então voltei e acrecentei eles aqui\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese') + [\"''\",\"``\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certifique-se que você já baixou o modelo com o comando: \n",
    "# python -m spacy download pt_core_news_md\n",
    "def load_docs():\n",
    "    model = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "    # primeiro passo: tokenizar por sentença cada texto\n",
    "    model_news = []\n",
    "    for idx,text in enumerate(text_news):\n",
    "        #print(idx)\n",
    "        model_news.append(model(text))\n",
    "    return model_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_news = load_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_news))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração das features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As primeira features a serem removidas são relacionadas ao \n",
    "# tamanho da sentença\n",
    "\n",
    "# a partir daí também podemos também pré-processar o texto\n",
    "tok_news = []\n",
    "# vamos armazenar as nossas features como dicionario e podemos transformar facilmente em \n",
    "# um dataframe\n",
    "sent_features = {'sent5':[],'sent5_10':[],'sent10':[]}\n",
    "\n",
    "for doc in model_news:\n",
    "    # esses serao os contadores para cada doc\n",
    "    sent5 = 0 \n",
    "    sent5_10 = 0\n",
    "    sent10 = 0\n",
    "    \n",
    "    tok_doc = []\n",
    "    for sent in doc.sents:\n",
    "        \n",
    "        # desconsiderando stopwords e palavras de tamanho 1\n",
    "        tok_lst = [tok for tok in sent if len(tok.text) > 1 and tok.text not in stopwords]\n",
    "\n",
    "        if len(tok_lst) <= 5:\n",
    "            sent5 += 1\n",
    "        elif len(tok_lst) > 5 and len(tok_lst) <= 10:\n",
    "            sent5_10 += 1\n",
    "        else:\n",
    "            sent10 += 1\n",
    "        tok_doc += tok_lst\n",
    "        \n",
    "    sent_features['sent5'].append(sent5)\n",
    "    sent_features['sent5_10'].append(sent5_10) \n",
    "    sent_features['sent10'].append(sent10)\n",
    "    \n",
    "    tok_news.append(tok_doc)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok_news[0]) # dando uma olhda nos tokens do primeiro documento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_features) # vamos ver como ficaram nossas features de sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com um histograma podemos ver melhor\n",
    "ax = plt.hist(sent_features['sent5'], 10)\n",
    "plt.title('Sentenças menores que 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com um histograma podemos ver melhor\n",
    "ax = plt.hist(sent_features['sent5_10'], 10)\n",
    "plt.title('Sentenças menores que 10 e maior que 5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# com um histograma podemos ver melhor\n",
    "ax = plt.hist(sent_features['sent10'], 10)\n",
    "plt.title('Sentenças maiores que 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisando os termos e extraindo outras features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos ver os tokens frequentes\n",
    "# tem algum que quero incluir em stop words?\n",
    "freq_term = {}\n",
    "for doc in model_news:\n",
    "    for tok in doc:\n",
    "        if tok.text in freq_term:\n",
    "            freq_term[tok.text] += 1\n",
    "        else:\n",
    "            freq_term[tok.text] = 1\n",
    "            \n",
    "freq_term_lst = list(freq_term.items())\n",
    "freq_term_lst.sort(key=lambda tup: tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_term_lst[:10]) # veja que são termos com baixa frequencia, podemos remove-los depois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_term_lst[-10:]) # veja como esta a acentuação..isso tem a ver com o encoding) \n",
    "# vejam que so eliminei os tokens de tamanho das sentenças, eles continuam no modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos usar o spacy para extrair as entidades nomeadas e nossas features #loc/#palavras, #per/#palavras e \n",
    "# #org/#palavras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_features = {'num_per':[],'num_loc':[],'num_org':[]}\n",
    "\n",
    "for doc in model_news:\n",
    "    \n",
    "    num_palavras = len(doc)\n",
    "    num_per = 0\n",
    "    num_loc = 0\n",
    "    num_org = 0\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER':\n",
    "            num_per += 1\n",
    "        if ent.label_ == 'LOC':\n",
    "            num_loc += 1\n",
    "        if ent.label_ == 'ORG':\n",
    "            num_org += 1\n",
    "                \n",
    "     \n",
    "    ner_features['num_per'].append(num_per / num_palavras)\n",
    "    ner_features['num_loc'].append(num_loc / num_palavras)\n",
    "    ner_features['num_org'].append(num_org / num_palavras)\n",
    "    \n",
    "print(ner_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agora para as features de pos tagging\n",
    "pos_features = {'VERB':[],'ADJ':[],'NOUN':[],'ADV':[]}\n",
    "\n",
    "for doc in model_news:\n",
    "    \n",
    "    verb = 0\n",
    "    adj = 0\n",
    "    noun = 0\n",
    "    adv = 0\n",
    "    num_palavras = len(doc)\n",
    "    \n",
    "    for tok in doc:\n",
    "        if tok.pos_ == 'VERB':\n",
    "            verb += 1\n",
    "        if tok.pos_ == 'ADJ':\n",
    "            adj += 1\n",
    "        if tok.pos_ == 'NOUN':\n",
    "            noun += 1\n",
    "        if tok.pos_ == 'ADV':\n",
    "            adv += 1\n",
    "\n",
    "    pos_features['VERB'].append(verb / num_palavras)\n",
    "    pos_features['ADJ'].append(adj / num_palavras)\n",
    "    pos_features['NOUN'].append(noun / num_palavras)\n",
    "    pos_features['ADV'].append(adv / num_palavras)\n",
    "    \n",
    "print(pos_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos contar os termos que estao no nosso dicionario racista\n",
    "\n",
    "# vamos carregar o dicionario\n",
    "dicionario_racista = open('dicRacista.txt','r').read().replace('\\n','').split(',')\n",
    "print(dicionario_racista[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "racista_features = {'racista':[]}\n",
    "for doc in model_news:\n",
    "    count = 0\n",
    "    for tok in doc:\n",
    "        if tok.text in dicionario_racista:\n",
    "            count += 1\n",
    "    racista_features['racista'].append(count)\n",
    "print(racista_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos extrair a quantidade de lexicos enviesados\n",
    "dicionario_vies = {'argumentativo':[],'pressuposicao':[],'possibilidade_necessidade':[],'opiniao_valoracao':[]}\n",
    "\n",
    "fd_dicionario_vies = open(\"bias_words.txt\",\"r\")\n",
    "for line in fd_dicionario_vies:\n",
    "    entry = line.replace(\"\\n\",\"\").split(\",\")\n",
    "    term = entry[0].strip()\n",
    "    type_term = entry[1].strip()\n",
    "    dicionario_vies[type_term].append(term)\n",
    "    \n",
    "print(dicionario_vies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depois da lista de termos podemos buscá-los em nossos textos\n",
    "def extrai_vies_features():\n",
    "    vies_features = {'argumentativo':[],'pressuposicao':[],'possibilidade_necessidade':[],'opiniao_valoracao':[]}\n",
    "\n",
    "    for idx, doc in enumerate(model_news):\n",
    "        # print(idx)\n",
    "        for type_term in dicionario_vies:\n",
    "            count = 0\n",
    "            for term in dicionario_vies[type_term]:\n",
    "\n",
    "                for sent in doc.sents:\n",
    "                    if term in sent.text.lower():\n",
    "                        count += 1\n",
    "                    \n",
    "            vies_features[type_term].append(count / len(doc))\n",
    "        \n",
    "    return vies_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time vies_features = extrai_vies_features() # demora um minuto e meio para 200 documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos agora analisar os erros ortograficos atraves de dicionario\n",
    "# embora existam tecnicas sofisticadas de detecção de erros ortográficos, \n",
    "# vamos usar o método de dicionário com o hunspell para simplificação da nossa tarefa\n",
    "hobj = hunspell.HunSpell('pt_BR.dic','pt_BR.aff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so para testar o hunpsell\n",
    "print(hobj.spell(\"casa\")) # <-- certo: True\n",
    "print(hobj.spell(\"caza\")) # <-- errado: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_features = {'erros_ort':[]}\n",
    "len_features = {'char_len':[]}\n",
    "for doc in model_news:\n",
    "    count = 0\n",
    "    len_count = 0\n",
    "    for tok in doc:\n",
    "        len_count += len(tok.text)\n",
    "        if not(hobj.spell(tok.text)):\n",
    "            count += 1\n",
    "    ort_features['erros_ort'].append(count / len(doc))\n",
    "    len_features['char_len'].append(len_count)\n",
    "print(ort_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora contar os lexicos positivos e negativos\n",
    "dicionario_pos_neg = {'positivos':[],'negativos':[]}\n",
    "fd_sent_lexicon = open('oplexicon_v3.0/lexico_v3.0.txt','r')\n",
    "\n",
    "for line in fd_sent_lexicon:\n",
    "    entry = line.replace('\\n','').split(',')\n",
    "    # ignorando emoticon e hashtags\n",
    "    if entry[1] != 'emot' and entry[1] != 'htag':\n",
    "        if entry[2].strip() == '-1':\n",
    "            dicionario_pos_neg['negativos'].append(entry[0])\n",
    "        if entry[2].strip() == '1':\n",
    "            dicionario_pos_neg['positivos'].append(entry[0])\n",
    "print(dicionario_pos_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_pos_neg():\n",
    "    pos_neg_features = {'positivos':[],'negativos':[]}\n",
    "\n",
    "    for doc in model_news:\n",
    "        positivos = 0\n",
    "        negativos = 0\n",
    "        for tok in doc:\n",
    "            if tok.text in dicionario_pos_neg['positivos']:\n",
    "                positivos += 1\n",
    "            if tok.text in dicionario_pos_neg['negativos']:\n",
    "                negativos += 1\n",
    "            \n",
    "        pos_neg_features['positivos'].append(positivos/len(doc))\n",
    "        pos_neg_features['negativos'].append(negativos/len(doc))\n",
    "    return pos_neg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pos_neg_features = extrai_pos_neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por fim, coletar os lexicos do emotaix\n",
    "# estou coletando apenas de 'Super category' (terceira coluna)\n",
    "dicionario_emotaix = {'ÓDIO':[],'AGRESSIVIDADE':[],'AFEIÇÃO':[],'GENTILEZA':[]}\n",
    "fd_emotaix = open('Emotaix-pt .csv','r')\n",
    "header = fd_emotaix.readline()\n",
    "\n",
    "for line in fd_emotaix:\n",
    "    entry = line.replace('\\n','').split(\",\")\n",
    "    if entry[2] in dicionario_emotaix:\n",
    "        dicionario_emotaix[entry[2]].append(entry[0])\n",
    "print(dicionario_emotaix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotaix_features = {'ÓDIO':[],'AGRESSIVIDADE':[],'AFEIÇÃO':[],'GENTILEZA':[]}\n",
    "\n",
    "for doc in model_news:\n",
    "    odio = 0\n",
    "    agressividade = 0\n",
    "    afeicao = 0\n",
    "    gentileza = 0\n",
    "    for tok in doc:\n",
    "        if tok.text in dicionario_emotaix['ÓDIO']:\n",
    "            odio += 1\n",
    "        if tok.text in dicionario_emotaix['AGRESSIVIDADE']:\n",
    "            agressividade += 1\n",
    "        if tok.text in dicionario_emotaix['AFEIÇÃO']:\n",
    "            afeicao += 1\n",
    "        if tok.text in dicionario_emotaix['GENTILEZA']:\n",
    "            gentileza += 1\n",
    "            \n",
    "    emotaix_features['ÓDIO'].append(odio / len(doc))\n",
    "    emotaix_features['AGRESSIVIDADE'].append(agressividade / len(doc))\n",
    "    emotaix_features['AFEIÇÃO'].append(afeicao / len(doc))\n",
    "    emotaix_features['GENTILEZA'].append(gentileza / len(doc))\n",
    "print(emotaix_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora juntar todas as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_features\n",
    "# ner_features \n",
    "# pos_features\n",
    "# racista_features\n",
    "# vies_features/subjetividade\n",
    "# ort_features \n",
    "# len_features\n",
    "# pos_neg_features\n",
    "# emotaix_features\n",
    "data = {}\n",
    "data_lst = [sent_features, ner_features, pos_features, \n",
    "            racista_features, vies_features, ort_features, \n",
    "            len_features, pos_neg_features, emotaix_features]\n",
    "\n",
    "for ftr in data_lst:\n",
    "    for k in ftr:\n",
    "        data[k] = ftr[k]\n",
    "        \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora vamos escrever em arquivo para guardar a extração \n",
    "# e não precisar rodar todo este script de novo\n",
    "#df.to_csv('fake_news_pos.csv',index=False)\n",
    "df = pd.read_csv(\"fake_news_pos_full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizando as features através de histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o searbon tem cinco temas: darkgrid, whitegrid, dark, white, e ticks\n",
    "sns.set_style(\"white\")\n",
    "ax = df.hist(bins=30, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizar com PCA e T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cmdlinetips.com/2018/03/pca-example-in-python-with-scikit-learn/\n",
    "\n",
    "\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df = pd.DataFrame(data = pc , \n",
    "        columns = ['PC1', 'PC2'])\n",
    "pc_df['Cluster'] = target\n",
    "pc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_explain = pd.DataFrame({'var':pca.explained_variance_ratio_,\n",
    "             'PC':['PC1','PC2']})\n",
    "sns.barplot(x='PC',y=\"var\", \n",
    "           data=df_explain, color=\"c\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para evitar apagar plots anteriores, podemos usar o subblot\n",
    "# Retorna o objeto da figura e também o eixo (axes) onde sera plotado\n",
    "# fica mais facil de manipular o nosso grafico tbm\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# coordenadas x\n",
    "# coordenadas y\n",
    "# c: sequencia de cores, no caso associamos a sequência de cores a nossa classe (fake ou não)\n",
    "# alpha: O valor de opacidade das cores, entre 0 (transparente) e 1 (totalmente opaco).\n",
    "# cmap -> color map: mapa de cores. Opções: https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "scatter = ax.scatter(pc_df.PC1, pc_df.PC2,\n",
    "                     c=pc_df.Cluster, \n",
    "                     alpha=0.6,\n",
    "                     cmap=plt.get_cmap(\"PiYG\"))\n",
    "plt.savefig('pca_fakenews_200.png')\n",
    "\n",
    "# opção caso queira colocar uma \"grade\" no seu gráfico    \n",
    "ax.grid(color='white', linestyle='solid')\n",
    "\n",
    "N = len(pc_df)\n",
    "labels = [\"Linha %d\" % d for d in range(N)]\n",
    "\n",
    "fig = plt.gcf()\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels)\n",
    "\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = tsne.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "\n",
    "colors_tsne = np.array(target)\n",
    "# duas classes apenas\n",
    "num_classes = len(np.unique(target))\n",
    "# pegando emprestado do seaborn uma paleta de cores\n",
    "palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "sc = ax.scatter(df_tsne[:,0], df_tsne[:,1], lw=0, s=40, c=palette[colors_tsne.astype(np.int)])\n",
    "plt.savefig('tsne_fakenews_200.png')\n",
    "\n",
    "fig = plt.gcf()\n",
    "tooltip = mpld3.plugins.PointLabelTooltip(sc, labels)\n",
    "\n",
    "mpld3.plugins.connect(fig, tooltip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_names[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificando os nossos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Só com o tamanho char_len -> baseline\n",
    "# vamos usar a validação cruzada com 5 folds, assim como no artigo\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, ShuffleSplit\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "df['target'] = target\n",
    "\n",
    "df = df.sample(frac=1,random_state=0)\n",
    "\n",
    "df.head()\n",
    "X = df.drop('target', 1)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7200, 23) (7200, 24)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC() # vamos usar os parâmetros que são default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores_svc = cross_validate(clf, X, y, cv=5, scoring=['accuracy','f1','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time 1.6036953926086426\n",
      "score_time 0.6173890590667724\n",
      "test_accuracy 0.9134722222222222\n",
      "train_accuracy 0.9836458333333333\n",
      "test_f1 0.9089024753624404\n",
      "train_f1 0.9837167613208683\n",
      "test_precision 0.9596080835141436\n",
      "train_precision 0.9794867656420653\n",
      "test_recall 0.8633333333333333\n",
      "train_recall 0.9879861111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_f1'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# vamos imprimir os nosso scores para cada fold\n",
    "for m in scores_svc:\n",
    "    print(m, np.mean(scores_svc[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# agora vamos testar com o classificador que o artigo usou\n",
    "# que tbm usou com os parâmetros padroes\n",
    "clf2 = svm.LinearSVC()\n",
    "scores_linearsvc = cross_validate(clf2, X, y, cv=5, scoring=['accuracy','f1','precision','recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time 0.583608865737915\n",
      "score_time 0.008779764175415039\n",
      "test_accuracy 0.7038888888888889\n",
      "train_accuracy 0.703923611111111\n",
      "test_f1 0.7895836766142019\n",
      "train_f1 0.7893784014792662\n",
      "test_precision 0.6849077169835934\n",
      "train_precision 0.6834492625009013\n",
      "test_recall 0.9838888888888888\n",
      "train_recall 0.9847222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_f1'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# vamos olhar nossos resultados\n",
    "# vamos imprimir os nosso scores para cada fold\n",
    "\n",
    "# aqui o recall aumentou e a precisao caiu, mas o nosso f1 aumentou um pouco\n",
    "for m in scores_linearsvc:\n",
    "    print(m, np.mean(scores_linearsvc[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sera que um metodo ensemble pode se dar melhor?\n",
    "clf3 = RandomForestClassifier(n_estimators=3)\n",
    "scores_forest = cross_validate(clf3, X, y, cv=5, scoring=['accuracy','f1','precision','recall'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_time 0.03225717544555664\n",
      "score_time 0.01051921844482422\n",
      "test_accuracy 0.9863888888888889\n",
      "train_accuracy 0.9972569444444443\n",
      "test_f1 0.9864244734945393\n",
      "train_f1 0.9972616300404009\n",
      "test_precision 0.9842796751133926\n",
      "train_precision 0.9958485281773198\n",
      "test_recall 0.9886111111111111\n",
      "train_recall 0.9986805555555553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_accuracy'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_f1'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_precision'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_recall'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "# vamos olhar nossos resultados\n",
    "# vamos imprimir os nosso scores para cada fold\n",
    "\n",
    "# olha só que legal, alcançamos resultados ainda melhores que o do artigo com um ensemble\n",
    "# contudo, lembrando que temos apenas 200 documentos!\n",
    "for m in scores_forest:\n",
    "    print(m, np.mean(scores_forest[m]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora vamos analisar as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quais features podem explicar o sucesso da nosso classificador de Random Forest?\n",
    "# vamos usar o shap para isso\n",
    "# https://github.com/slundberg/shap\n",
    "import shap\n",
    "import sklearn\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para usar o shap, vamos aqui usar o dado todo para buscar as nossas explicações\n",
    "# assim precisamos treinar o classificador clf3 no nosso dado\n",
    "\n",
    "X = df.drop('target', 1)\n",
    "y = df.target\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = RandomForestClassifier(n_estimators=3)\n",
    "clf4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apenas um experimento da professora com Curva de precisão-recall\n",
    "y_pred = clf4.predict_proba(X_valid)\n",
    "pos_probs = y_pred[:, 1]\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_valid, pos_probs)\n",
    "auc_score = auc(recall, precision)\n",
    "print('No Skill PR AUC: %.3f' % auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(clf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "#explainer.shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "fig = plt.gcf()\n",
    "shap.summary_plot(shap_values, X_train, feature_names=df.columns,plot_size=(15,7))\n",
    "fig.savefig('shap_randomforest.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outra forma de calcular importância de feature com RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a forest and compute the impurity-based feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=3,\n",
    "                              random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0) # desvio padrao da importancia de feature para cada arvore no nosso ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the impurity-based feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "        color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
