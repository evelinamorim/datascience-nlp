{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certifique-se que você tem o corpus tentando fazer o download. \n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import machado\n",
    "import nltk\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "corpus_machado = []\n",
    "for f_id in machado.fileids():\n",
    "    tok_lst = simple_preprocess(machado.raw(f_id))\n",
    "    tok_lst = [tok for tok in tok_lst if tok not in stopwords]\n",
    "    corpus_machado.append(tok_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicionario = Dictionary(corpus_machado) # modulo que lida com dicionario e suas funcionalidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converte o corpus para o formato de saco de palavras\n",
    "corpus_machado_bow = [dicionario.doc2bow(line) for line in corpus_machado]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# alguns parametros\n",
    "num_topics = 5\n",
    "iterations = 40\n",
    "\n",
    "temp = dicionario[0]  # Isso é apenas para \"carregar\" o dicionario\n",
    "id2word = dicionario.id2token\n",
    "\n",
    "model = LdaModel(corpus=corpus_machado_bow,\n",
    "                 id2word=id2word,\n",
    "                num_topics=num_topics,\n",
    "                iterations=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.006*\"disse\" + 0.004*\"tudo\" + 0.004*\"casa\" + 0.004*\"ser\" + 0.003*\"nada\" + '\n",
      "  '0.003*\"outro\" + 0.003*\"dois\" + 0.003*\"tempo\" + 0.003*\"tão\" + 0.003*\"outra\"'),\n",
      " (1,\n",
      "  '0.004*\"ainda\" + 0.004*\"casa\" + 0.004*\"disse\" + 0.003*\"ser\" + 0.003*\"tempo\" '\n",
      "  '+ 0.003*\"olhos\" + 0.003*\"porque\" + 0.003*\"tudo\" + 0.003*\"homem\" + '\n",
      "  '0.003*\"nada\"'),\n",
      " (2,\n",
      "  '0.006*\"disse\" + 0.003*\"vez\" + 0.003*\"coisa\" + 0.003*\"tudo\" + 0.003*\"dia\" + '\n",
      "  '0.003*\"ainda\" + 0.003*\"tão\" + 0.003*\"tempo\" + 0.003*\"lo\" + 0.003*\"casa\"'),\n",
      " (3,\n",
      "  '0.004*\"ser\" + 0.004*\"disse\" + 0.004*\"tudo\" + 0.003*\"casa\" + 0.003*\"olhos\" + '\n",
      "  '0.003*\"tempo\" + 0.003*\"outra\" + 0.003*\"dia\" + 0.003*\"dois\" + 0.003*\"coisa\"'),\n",
      " (4,\n",
      "  '0.004*\"ainda\" + 0.003*\"disse\" + 0.003*\"coisa\" + 0.003*\"ser\" + 0.003*\"dia\" + '\n",
      "  '0.003*\"bem\" + 0.003*\"tempo\" + 0.002*\"pode\" + 0.002*\"dias\" + 0.002*\"outro\"')]\n"
     ]
    }
   ],
   "source": [
    "# no caso do lda, a lematização ajuda a criar tópicos mais uniformes\n",
    "pprint(model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc0 = corpus_machado_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos usar o doc_topics como representacao do nosso documento\n",
    "doc_topics, word_topics, phi_values = model.get_document_topics(doc0, per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.16908072), (1, 0.18243022), (2, 0.55894125), (3, 0.06974466), (4, 0.019803103)]\n"
     ]
    }
   ],
   "source": [
    "print(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
