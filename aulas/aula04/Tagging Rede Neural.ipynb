{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging com spacy\n",
    "\n",
    "Aqui temos uma aplicação \"caixa preta\" de redes neurais com POS tagging. Contudo é \n",
    "possível fazer uma implementação com redes neurais usando alguma biblioteca como keras ou pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.__version__) \n",
    "# a versão é importante, pois nem toda versão do spacy contém o modelo\n",
    "# pt_core_news_md. A versão que foi usada em aula foi 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# certifique-se que você já baixou o modelo com o comando: \n",
    "# python -m spacy download pt_core_news_md\n",
    "\n",
    "# Descrição desse modelo da página do spacy (tradução livre):\n",
    "# Cnn multi-tarefa em português treinada nos corpora Bosque e WikiNER. \n",
    "# Atribui vetor de palavras, POS tags, parser de dependência, e entidades nomeadas.\n",
    "# Os vetores de palavras foram terinados com o FastText CBOW na Wikipedia e no OSCAR (Common Crawl).\n",
    "model = spacy.load(\"pt_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos aplicar esse modelo a um texto do corpus Machado\n",
    "from nltk.corpus import machado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contos/macn001.txt', 'contos/macn002.txt', 'contos/macn003.txt', 'contos/macn004.txt', 'contos/macn005.txt']\n"
     ]
    }
   ],
   "source": [
    "# checando os nomes dos meus textos\n",
    "print(machado.fileids()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos pegar so o primeiro conto e rotular\n",
    "conto_machado = machado.raw(machado.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o modelo do spacy transforma em um objeto do tipo documento\n",
    "# por enquanto vamos só checar as pos tags\n",
    "doc_conto = model(conto_machado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lst = []\n",
    "for token in doc_conto:\n",
    "    # print(dir(token))\n",
    "    pos_lst.append((token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n\\n', 'SPACE'), ('Era', 'AUX'), ('conveniente', 'ADJ'), ('ao', 'DET'), ('romance', 'NOUN'), ('que', 'PRON'), ('o', 'DET'), ('leitor', 'NOUN'), ('\\n', 'SPACE'), ('ficasse', 'VERB'), ('muito', 'ADV'), ('tempo', 'NOUN'), ('sem', 'SCONJ'), ('saber', 'VERB'), ('quem', 'PRON'), ('era', 'AUX'), ('Miss', 'PROPN'), ('Dollar', 'PROPN'), ('.', 'PUNCT'), ('Mas', 'CCONJ'), ('por', 'ADP'), ('outro', 'DET'), ('lado', 'NOUN'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('sem', 'ADP'), ('a', 'DET'), ('apresentação', 'NOUN'), ('de', 'ADP'), ('Miss', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "# Vamos checar a nossa lista de pos tags, apenas uma parte, pois o texto é grande. \n",
    "# ignoramos o inicio que é só um cabeçalho do conto\n",
    "print(pos_lst[120:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER no spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lst = []\n",
    "for ent in doc_conto.ents:\n",
    "    ner_lst.append((ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Contos Fluminenses', 'MISC'), ('Contos Fluminenses', 'LOC'), ('Obra Completa', 'MISC'), ('Machado de Assis', 'PER'), ('Rio de Janeiro', 'LOC'), ('Nova Aguilar', 'LOC'), ('Editora Garnier', 'ORG'), ('Rio de Janeiro', 'LOC'), ('MISS', 'ORG'), ('SOARES', 'ORG'), ('SEGREDO DE AUGUSTA', 'ORG'), ('CONFISSÕES DE UMA', 'ORG'), ('RETA', 'LOC'), ('LINHA CURVA', 'LOC'), ('FREI', 'ORG'), ('SIMÃO', 'ORG'), ('MISS', 'ORG'), ('Capítulo iv', 'MISC'), ('CAPÍTULO VIII', 'MISC'), ('CAPÍTULO PRIMEIRO', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print(ner_lst[:20]) # veja que só apareceram as palavras que possuem alguma classificação NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vc pode tbm pegar as informacoes do token e das entidades\n",
    "doc_conto = model(conto_machado)\n",
    "pos_ner_lst = []\n",
    "for token in doc_conto:\n",
    "    pos_ner_lst.append((token.text, token.pos_, token.ent_type_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\n\\n', 'SPACE', ''), ('Era', 'AUX', ''), ('conveniente', 'ADJ', ''), ('ao', 'DET', ''), ('romance', 'NOUN', ''), ('que', 'PRON', ''), ('o', 'DET', ''), ('leitor', 'NOUN', ''), ('\\n', 'SPACE', ''), ('ficasse', 'VERB', ''), ('muito', 'ADV', ''), ('tempo', 'NOUN', ''), ('sem', 'SCONJ', ''), ('saber', 'VERB', ''), ('quem', 'PRON', ''), ('era', 'AUX', ''), ('Miss', 'PROPN', 'ORG'), ('Dollar', 'PROPN', 'ORG'), ('.', 'PUNCT', ''), ('Mas', 'CCONJ', ''), ('por', 'ADP', ''), ('outro', 'DET', ''), ('lado', 'NOUN', ''), (',', 'PUNCT', ''), ('\\n', 'SPACE', ''), ('sem', 'ADP', ''), ('a', 'DET', ''), ('apresentação', 'NOUN', ''), ('de', 'ADP', ''), ('Miss', 'PROPN', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_ner_lst[120:150]) # nesse caso, o problema eh que tem entidade que esta em mais de um token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
