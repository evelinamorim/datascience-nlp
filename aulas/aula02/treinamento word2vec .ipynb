{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando um modelo word2vec usando o gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse notebook foi baseado no seguinte tutorial https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial \n",
    "\n",
    "import nltk\n",
    "# nltk.download(\"machado\") # certifique-se que o corpus machado existe no seu computador\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import machado # importando o corpus machado de livros do Machado de Assis\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o algoritmo word2vec processa as palavras por sentença. Então precisamos dividir os textos\n",
    "# como sentenças para alimentar o nosso modelo. A função sent_tokenize do ntlk faz isso.\n",
    "corpus_machado = []\n",
    "for f_id in machado.fileids():\n",
    "    sent_lst = sent_tokenize(machado.raw(f_id))\n",
    "    for s in sent_lst:\n",
    "        corpus_machado.append(simple_preprocess(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros\n",
    "# min_count = Ignora todas as palavras com frequencia absoluta total\n",
    "# menor que isso\n",
    "# window = A distância máxima entre a palavra corrente e palavra predita em uma sentença. Aqui, duas palavras\n",
    "# palavras anteriores e duas palavras anteriores a palavra corrente.\n",
    "# size = Dimensão dos vetores densos ou word embeddings. \n",
    "w2v_model = Word2Vec(window=2,\n",
    "                     size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o word2vec demanda a construção de uma tabela representando o vocabulário do nosso corpus.\n",
    "# Ou seja, simplesmente \"digerir\" todas as palavras e filtrar as palavras únicas, e fazer uma\n",
    "# contagem básica delas.\n",
    "w2v_model.build_vocab(corpus_machado, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8815665, 11281480)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parametros\n",
    "# total_examples = Contagem de sentenças\n",
    "# epochs = Número de iterações (épocas) sobre o corpus\n",
    "\n",
    "w2v_model.train(corpus_machado, total_examples=w2v_model.corpus_count, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# salvando o nosso modelo. Podemos carregar o modelo novamente, em outro código, \n",
    "# com o seguinte comando:  model = Word2Vec.load(\"word2vec.model\")\n",
    "w2v_model.save(\"w2v_machado.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06168859,  0.00802544,  0.0021128 , -0.07074621, -0.06244523,\n",
       "        0.0096143 , -0.12688795,  0.09584288,  0.06395809,  0.26928422,\n",
       "        0.01020958, -0.07030839,  0.16444029,  0.00537581, -0.08293233,\n",
       "       -0.13409011, -0.04957008, -0.03570518, -0.05825284, -0.10404382,\n",
       "        0.19081631,  0.02699185, -0.1579889 , -0.16260406,  0.03763396,\n",
       "        0.03231732,  0.0945655 , -0.00065857,  0.1299227 ,  0.15360348,\n",
       "        0.07988261,  0.0178228 ,  0.15269141, -0.07695309, -0.0381519 ,\n",
       "       -0.02446295, -0.07362824,  0.05914355,  0.03226122,  0.08915292,\n",
       "        0.07149906, -0.04814176, -0.00824675, -0.06472199, -0.04262403,\n",
       "        0.18127804,  0.00960096,  0.07953376, -0.09608218, -0.01583127],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['abade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('escrevente', 0.9208676815032959),\n",
       " ('encargo', 0.9198858737945557),\n",
       " ('alaúde', 0.9143734574317932),\n",
       " ('art', 0.9119309186935425),\n",
       " ('lixo', 0.9088165163993835),\n",
       " ('folhetinista', 0.9082868695259094),\n",
       " ('tapamento', 0.9078084230422974),\n",
       " ('vencedor', 0.9069023132324219),\n",
       " ('amazonas', 0.9067910313606262),\n",
       " ('ótimo', 0.9067627787590027)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# em outras aula veremos como é calculada a similaridade, mas veja so já a capacidade \n",
    "# do word2vec\n",
    "# repare que como temos um corpus pequeno, os resultados não são tão bons assim\n",
    "w2v_model.most_similar(positive=['abade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('artigo', 0.8474695682525635),\n",
       " ('volume', 0.843697726726532),\n",
       " ('papel', 0.8359150886535645),\n",
       " ('romance', 0.8116697669029236),\n",
       " ('processo', 0.8042157292366028),\n",
       " ('discurso', 0.7978098392486572),\n",
       " ('cantinho', 0.7912461757659912),\n",
       " ('programa', 0.782477855682373),\n",
       " ('testamento', 0.7818537950515747),\n",
       " ('lance', 0.7812900543212891)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos tentar outra palavra? Essa palavra já está mais parecida com o que achamos intuitiva\n",
    "# devido ao dominio. \n",
    "w2v_model.most_similar(positive=['livro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
